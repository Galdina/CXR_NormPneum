{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /Users/alphonsowoodbury/opt/miniconda3/envs/learn-env/lib/python3.6/site-packages (4.2.0.34)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /Users/alphonsowoodbury/opt/miniconda3/envs/learn-env/lib/python3.6/site-packages (from opencv-python) (1.16.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#!{sys.executable} -m pip install opencv-python\n",
    "\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_folder = '../data/test'\n",
    "train_folder = '../data/train'\n",
    "val_folder = '../data/val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 624 images belonging to 2 classes.\n",
      "Found 16 images belonging to 2 classes.\n",
      "Found 5216 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# get all the data in the directory split/test (180 images), and reshape them\n",
    "test_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "        test_folder, \n",
    "        target_size=(64,64), batch_size = 627) \n",
    "\n",
    "# get all the data in the directory split/validation (200 images), and reshape them\n",
    "val_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "        val_folder, \n",
    "        target_size=(64,64), batch_size = 19)\n",
    "\n",
    "# get all the data in the directory split/train (542 images), and reshape them\n",
    "train_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "        train_folder, \n",
    "        target_size=(64,64), batch_size=5219)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, train_labels = next(train_generator)\n",
    "test_images, test_labels = next(test_generator)\n",
    "val_images, val_labels = next(val_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 5216\n",
      "Number of testing samples: 624\n",
      "Number of validation samples: 16\n",
      "train_images shape: (5216, 64, 64, 3)\n",
      "train_labels shape: (5216, 2)\n",
      "test_images shape: (624, 64, 64, 3)\n",
      "test_labels shape: (624, 2)\n",
      "val_images shape: (16, 64, 64, 3)\n",
      "val_labels shape: (16, 2)\n"
     ]
    }
   ],
   "source": [
    "# Explore your dataset again\n",
    "m_train = train_images.shape[0]\n",
    "num_px = train_images.shape[1]\n",
    "m_test = test_images.shape[0]\n",
    "m_val = val_images.shape[0]\n",
    "\n",
    "print (\"Number of training samples: \" + str(m_train))\n",
    "print (\"Number of testing samples: \" + str(m_test))\n",
    "print (\"Number of validation samples: \" + str(m_val))\n",
    "print (\"train_images shape: \" + str(train_images.shape))\n",
    "print (\"train_labels shape: \" + str(train_labels.shape))\n",
    "print (\"test_images shape: \" + str(test_images.shape))\n",
    "print (\"test_labels shape: \" + str(test_labels.shape))\n",
    "print (\"val_images shape: \" + str(val_images.shape))\n",
    "print (\"val_labels shape: \" + str(val_labels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5216, 12288)\n",
      "(624, 12288)\n",
      "(16, 12288)\n"
     ]
    }
   ],
   "source": [
    "train_img = train_images.reshape(train_images.shape[0], -1)\n",
    "test_img = test_images.reshape(test_images.shape[0], -1)\n",
    "val_img = val_images.reshape(val_images.shape[0], -1)\n",
    "\n",
    "print(train_img.shape)\n",
    "print(test_img.shape)\n",
    "print(val_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = np.reshape(train_labels[:,0], (5216,1))\n",
    "test_y = np.reshape(test_labels[:,0], (624,1))\n",
    "val_y = np.reshape(val_labels[:,0], (16,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline fully-connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "import keras_metrics\n",
    "np.random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(20, activation='relu', input_shape=(12288,))) # 2 hidden layers\n",
    "model.add(layers.Dense(7, activation='relu'))\n",
    "model.add(layers.Dense(5, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5216 samples, validate on 16 samples\n",
      "Epoch 1/50\n",
      "5216/5216 [==============================] - ETA: 0s - loss: 0.1496 - acc: 0.940 - 1s 194us/step - loss: 0.1481 - acc: 0.9408 - val_loss: 0.7925 - val_acc: 0.6875\n",
      "Epoch 2/50\n",
      "5216/5216 [==============================] - 1s 109us/step - loss: 0.1321 - acc: 0.9500 - val_loss: 0.9314 - val_acc: 0.6250\n",
      "Epoch 3/50\n",
      "5216/5216 [==============================] - 1s 105us/step - loss: 0.1514 - acc: 0.9404 - val_loss: 0.4808 - val_acc: 0.8125\n",
      "Epoch 4/50\n",
      "5216/5216 [==============================] - 1s 103us/step - loss: 0.1268 - acc: 0.9523 - val_loss: 0.5393 - val_acc: 0.8125\n",
      "Epoch 5/50\n",
      "5216/5216 [==============================] - 1s 106us/step - loss: 0.1271 - acc: 0.9473 - val_loss: 0.3268 - val_acc: 0.8750\n",
      "Epoch 6/50\n",
      "5216/5216 [==============================] - 1s 104us/step - loss: 0.1235 - acc: 0.9551 - val_loss: 0.7370 - val_acc: 0.6875\n",
      "Epoch 7/50\n",
      "5216/5216 [==============================] - 1s 103us/step - loss: 0.1279 - acc: 0.9496 - val_loss: 0.2157 - val_acc: 0.8750\n",
      "Epoch 8/50\n",
      "5216/5216 [==============================] - 1s 109us/step - loss: 0.1239 - acc: 0.9519 - val_loss: 0.2409 - val_acc: 0.9375\n",
      "Epoch 9/50\n",
      "5216/5216 [==============================] - 1s 104us/step - loss: 0.1381 - acc: 0.9469 - val_loss: 0.2569 - val_acc: 0.9375\n",
      "Epoch 10/50\n",
      "5216/5216 [==============================] - 1s 105us/step - loss: 0.1178 - acc: 0.9557 - val_loss: 0.2201 - val_acc: 0.9375\n",
      "Epoch 11/50\n",
      "5216/5216 [==============================] - 1s 110us/step - loss: 0.1244 - acc: 0.9500 - val_loss: 0.3754 - val_acc: 0.8125\n",
      "Epoch 12/50\n",
      "5216/5216 [==============================] - 1s 110us/step - loss: 0.1105 - acc: 0.9592 - val_loss: 0.2532 - val_acc: 0.9375\n",
      "Epoch 13/50\n",
      "5216/5216 [==============================] - 1s 109us/step - loss: 0.1153 - acc: 0.9576 - val_loss: 0.2865 - val_acc: 0.9375\n",
      "Epoch 14/50\n",
      "5216/5216 [==============================] - 1s 114us/step - loss: 0.1087 - acc: 0.9580 - val_loss: 0.1902 - val_acc: 0.9375\n",
      "Epoch 15/50\n",
      "5216/5216 [==============================] - 1s 112us/step - loss: 0.1150 - acc: 0.9559 - val_loss: 0.3030 - val_acc: 0.9375\n",
      "Epoch 16/50\n",
      "5216/5216 [==============================] - 1s 110us/step - loss: 0.1077 - acc: 0.9584 - val_loss: 0.4149 - val_acc: 0.8125\n",
      "Epoch 17/50\n",
      "5216/5216 [==============================] - 1s 114us/step - loss: 0.1044 - acc: 0.9599 - val_loss: 0.3163 - val_acc: 0.9375\n",
      "Epoch 18/50\n",
      "5216/5216 [==============================] - 1s 110us/step - loss: 0.1089 - acc: 0.9578 - val_loss: 0.2226 - val_acc: 0.9375\n",
      "Epoch 19/50\n",
      "5216/5216 [==============================] - 1s 110us/step - loss: 0.1055 - acc: 0.9609 - val_loss: 0.7021 - val_acc: 0.6875\n",
      "Epoch 20/50\n",
      "5216/5216 [==============================] - 1s 112us/step - loss: 0.1092 - acc: 0.9578 - val_loss: 0.5350 - val_acc: 0.8125\n",
      "Epoch 21/50\n",
      "5216/5216 [==============================] - 1s 115us/step - loss: 0.1035 - acc: 0.9586 - val_loss: 0.4525 - val_acc: 0.8125\n",
      "Epoch 22/50\n",
      "5216/5216 [==============================] - 1s 115us/step - loss: 0.1155 - acc: 0.9582 - val_loss: 0.2611 - val_acc: 0.9375\n",
      "Epoch 23/50\n",
      "5216/5216 [==============================] - 1s 116us/step - loss: 0.0976 - acc: 0.9663 - val_loss: 0.3636 - val_acc: 0.8125\n",
      "Epoch 24/50\n",
      "5216/5216 [==============================] - 1s 114us/step - loss: 0.1041 - acc: 0.9613 - val_loss: 0.2535 - val_acc: 0.9375\n",
      "Epoch 25/50\n",
      "5216/5216 [==============================] - 1s 116us/step - loss: 0.0960 - acc: 0.9657 - val_loss: 0.1846 - val_acc: 0.9375\n",
      "Epoch 26/50\n",
      "5216/5216 [==============================] - 1s 110us/step - loss: 0.1095 - acc: 0.9609 - val_loss: 0.3815 - val_acc: 0.8125\n",
      "Epoch 27/50\n",
      "5216/5216 [==============================] - 1s 111us/step - loss: 0.0991 - acc: 0.9617 - val_loss: 0.7042 - val_acc: 0.6875\n",
      "Epoch 28/50\n",
      "5216/5216 [==============================] - 1s 116us/step - loss: 0.1035 - acc: 0.9622 - val_loss: 0.4843 - val_acc: 0.6875\n",
      "Epoch 29/50\n",
      "5216/5216 [==============================] - 1s 118us/step - loss: 0.0963 - acc: 0.9653 - val_loss: 0.9011 - val_acc: 0.6250\n",
      "Epoch 30/50\n",
      "5216/5216 [==============================] - 1s 120us/step - loss: 0.0925 - acc: 0.9649 - val_loss: 0.1984 - val_acc: 0.9375\n",
      "Epoch 31/50\n",
      "5216/5216 [==============================] - 1s 122us/step - loss: 0.0988 - acc: 0.9636 - val_loss: 0.3319 - val_acc: 0.8750\n",
      "Epoch 32/50\n",
      "5216/5216 [==============================] - 1s 136us/step - loss: 0.0926 - acc: 0.9643 - val_loss: 0.2247 - val_acc: 0.9375\n",
      "Epoch 33/50\n",
      "5216/5216 [==============================] - 1s 122us/step - loss: 0.0937 - acc: 0.9661 - val_loss: 0.3822 - val_acc: 0.8125\n",
      "Epoch 34/50\n",
      "5216/5216 [==============================] - 1s 129us/step - loss: 0.0823 - acc: 0.9693 - val_loss: 0.6650 - val_acc: 0.6875\n",
      "Epoch 35/50\n",
      "5216/5216 [==============================] - 1s 127us/step - loss: 0.0891 - acc: 0.9657 - val_loss: 0.9967 - val_acc: 0.6250\n",
      "Epoch 36/50\n",
      "5216/5216 [==============================] - 1s 126us/step - loss: 0.0872 - acc: 0.9680 - val_loss: 0.2953 - val_acc: 0.8750\n",
      "Epoch 37/50\n",
      "5216/5216 [==============================] - 1s 129us/step - loss: 0.0953 - acc: 0.9643 - val_loss: 0.1733 - val_acc: 0.9375\n",
      "Epoch 38/50\n",
      "5216/5216 [==============================] - 1s 117us/step - loss: 0.0936 - acc: 0.9634 - val_loss: 0.3555 - val_acc: 0.8125\n",
      "Epoch 39/50\n",
      "5216/5216 [==============================] - 1s 114us/step - loss: 0.0964 - acc: 0.9622 - val_loss: 0.5474 - val_acc: 0.8125\n",
      "Epoch 40/50\n",
      "5216/5216 [==============================] - 1s 112us/step - loss: 0.0868 - acc: 0.9670 - val_loss: 0.3516 - val_acc: 0.8750\n",
      "Epoch 41/50\n",
      "5216/5216 [==============================] - 1s 114us/step - loss: 0.0859 - acc: 0.9666 - val_loss: 0.2981 - val_acc: 0.8750\n",
      "Epoch 42/50\n",
      "5216/5216 [==============================] - 1s 118us/step - loss: 0.0799 - acc: 0.9716 - val_loss: 0.5358 - val_acc: 0.8125\n",
      "Epoch 43/50\n",
      "5216/5216 [==============================] - 1s 117us/step - loss: 0.0919 - acc: 0.9632 - val_loss: 0.3804 - val_acc: 0.8125\n",
      "Epoch 44/50\n",
      "5216/5216 [==============================] - 1s 114us/step - loss: 0.0791 - acc: 0.9689 - val_loss: 0.2167 - val_acc: 0.9375\n",
      "Epoch 45/50\n",
      "5216/5216 [==============================] - 1s 119us/step - loss: 0.0813 - acc: 0.9691 - val_loss: 0.1937 - val_acc: 0.9375\n",
      "Epoch 46/50\n",
      "5216/5216 [==============================] - 1s 116us/step - loss: 0.0816 - acc: 0.9693 - val_loss: 0.2938 - val_acc: 0.8750\n",
      "Epoch 47/50\n",
      "5216/5216 [==============================] - 1s 117us/step - loss: 0.0841 - acc: 0.9703 - val_loss: 0.1715 - val_acc: 0.9375\n",
      "Epoch 48/50\n",
      "5216/5216 [==============================] - 1s 118us/step - loss: 0.0763 - acc: 0.9714 - val_loss: 0.1477 - val_acc: 0.9375\n",
      "Epoch 49/50\n",
      "5216/5216 [==============================] - 1s 119us/step - loss: 0.0791 - acc: 0.9707 - val_loss: 0.6865 - val_acc: 0.7500\n",
      "Epoch 50/50\n",
      "5216/5216 [==============================] - 1s 115us/step - loss: 0.0811 - acc: 0.9707 - val_loss: 0.1406 - val_acc: 0.9375\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='sgd',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "histoire = model.fit(train_img,\n",
    "                    train_y,\n",
    "                    epochs=50,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(val_img, val_y),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5216/5216 [==============================] - 0s 53us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_img, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "624/624 [==============================] - 0s 59us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(test_img, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.07977211311390978, 0.9702837423312883]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7531664157525088, 0.7916666666666666]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_metrics\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                        input_shape=(64,64,  3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Dense(4, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=[keras_metrics.precision(), keras_metrics.recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_37 to have 4 dimensions, but got array with shape (5216, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-e1f5a4be457e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                     verbose=1)\n\u001b[0m",
      "\u001b[0;32m~/opt/miniconda3/envs/learn-env/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/learn-env/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/learn-env/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    126\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    129\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_37 to have 4 dimensions, but got array with shape (5216, 1)"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_images,\n",
    "                    train_y,\n",
    "                    epochs=1,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(val_images, val_y),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5216/5216 [==============================] - 126s 24ms/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_images, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "624/624 [==============================] - 16s 26ms/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(test_images, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.28860043190373963, 0.8989266546602033, 0.7494407158277822]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.43435644950622165, 0.9133858260524521, 0.4957264955146468]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: tf: command not found\n"
     ]
    }
   ],
   "source": [
    "! tf --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_metrics\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                        input_shape=(256,256,  3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(32, (4, 4), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=[keras_metrics.precision(), keras_metrics.recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5216 samples, validate on 16 samples\n",
      "Epoch 1/1\n",
      " 256/5216 [>.............................] - ETA: 5:02 - loss: 0.6088 - precision: 0.0000e+00 - recall: 0.0000e+00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_images,\n",
    "                    train_y,\n",
    "                    epochs=1,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(val_images, val_y),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_train = model.evaluate(train_images, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test = model.evaluate(test_images, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:learn-env] *",
   "language": "python",
   "name": "conda-env-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
